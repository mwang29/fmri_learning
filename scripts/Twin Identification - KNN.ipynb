{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "from scipy.spatial import distance\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utri2mat(utri):\n",
    "    n = int(-1 + np.sqrt(1 + 8 * len(utri))) // 2\n",
    "    iu1 = np.triu_indices(n+1,1)\n",
    "    ret = np.empty((n+1, n+1))\n",
    "    ret[iu1] = utri\n",
    "    ret.T[iu1] = utri\n",
    "    np.fill_diagonal(ret, 1)\n",
    "    return ret\n",
    "\n",
    "def get_data(test_idx, retest_idx, parc, twin='D'):\n",
    "    '''\n",
    "    Navigates through file tree and extracts FCs with optional reconstruction\n",
    "    '''\n",
    "    # Yeo ordering\n",
    "    master_dir = '../data/twins'\n",
    "    tasks = ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']\n",
    "    FC, test, retest = {}, {}, {}\n",
    "    for task in tasks:\n",
    "        temp_parc = {}\n",
    "        task_dir = master_dir + f'/{task.upper()}/origmat_{twin}Z_schaefer{parc}_tests.mat'\n",
    "        f = h5py.File(task_dir, 'r')\n",
    "        for k, v in f.items():\n",
    "            temp_parc[k] = np.array(v)\n",
    "        test[task] = temp_parc['orig_mat'][test_idx[task]]\n",
    "        temp_parc = {}\n",
    "        task_dir = master_dir + f'/{task.upper()}/origmat_{twin}Z_schaefer{parc}_retests.mat'\n",
    "        f = h5py.File(task_dir, 'r')\n",
    "        for k, v in f.items():\n",
    "            temp_parc[k] = np.array(v)\n",
    "        retest[task] = temp_parc['orig_mat'][retest_idx[task]]\n",
    "        FC[task] = np.concatenate((test[task], retest[task])) \n",
    "    return FC\n",
    "\n",
    "\n",
    "def q1invm(q1, eig_thresh=0):\n",
    "    U, S, V = scipy.linalg.svd(q1)\n",
    "    s = np.diag(S)\n",
    "    s[s < eig_thresh] = eig_thresh\n",
    "    S = np.diag(s ** (-1 / 2))\n",
    "    Q1_inv_sqrt = U * S * np.transpose(V)\n",
    "    Q1_inv_sqrt = (Q1_inv_sqrt + np.transpose(Q1_inv_sqrt)) / 2\n",
    "    return Q1_inv_sqrt\n",
    "\n",
    "\n",
    "def qlog(q):\n",
    "    U, S, V = scipy.linalg.svd(q)\n",
    "    s = np.diag(S)\n",
    "    S = np.diag(np.log(s))\n",
    "    Q = U * S * np.transpose(V)\n",
    "    return Q\n",
    "\n",
    "\n",
    "def tangential(all_FC, ref):\n",
    "    # Regularization for riemann\n",
    "    if ref in ['riemann', 'kullback_sym', 'logeuclid']: \n",
    "        print(\"Adding regularization!\")\n",
    "        eye_mat = np.eye(all_FC.shape[1])\n",
    "        scaling_mat = np.repeat(eye_mat[None, ...], all_FC.shape[0], axis=0)\n",
    "        all_FC += scaling_mat\n",
    "    u, s, vh = np.linalg.svd(all_FC[0], full_matrices=True)\n",
    "    Cg = mean_covariance(all_FC, metric=ref)\n",
    "    Q1_inv_sqrt = q1invm(Cg)\n",
    "    Q = Q1_inv_sqrt @ all_FC @ Q1_inv_sqrt\n",
    "    tangent_FC = np.array([qlog(a) for a in Q])\n",
    "    return tangent_FC\n",
    "\n",
    "\n",
    "def pca_recon(FC, pctComp=None):\n",
    "    '''\n",
    "    Reconstructs FC based on number of principle components\n",
    "    '''\n",
    "    if pctComp is None:\n",
    "        return FC\n",
    "    nRegions = FC.shape[1]\n",
    "    FC = np.reshape(FC, (FC.shape[0], -1))\n",
    "    nComp = int(FC.shape[0] * pctComp)\n",
    "    mu = np.mean(FC, axis=0)\n",
    "    pca_rest = sklearn.decomposition.PCA()\n",
    "    pca_rest.fit(FC)\n",
    "    SCORES = pca_rest.transform(FC)[:, :nComp]\n",
    "    COEFFS = pca_rest.components_[:nComp, :]\n",
    "    FC_recon = np.dot(SCORES, COEFFS)\n",
    "    del SCORES, COEFFS\n",
    "    FC_recon += mu\n",
    "    FC_recon = np.reshape(FC_recon, (FC.shape[0], nRegions, nRegions))\n",
    "    return FC_recon\n",
    "\n",
    "def utri2mat(utri):\n",
    "    n = int(-1 + np.sqrt(1 + 8 * len(utri))) // 2\n",
    "    iu1 = np.tril_indices(n+1,-1)\n",
    "    ret = np.empty((n+1, n+1))\n",
    "    ret[iu1] = utri\n",
    "    ret.T[iu1] = utri\n",
    "    np.fill_diagonal(ret, 1)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twin Subject ID Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']\n",
    "test_subj_ids, retest_subj_ids = {}, {}\n",
    "for task in tasks:\n",
    "    test_subj_vec, retest_subj_vec = {}, {}\n",
    "    master_dir = '../data/twins'\n",
    "    test_subj_dir = master_dir + f'/{task.upper()}/subjvec_test_DZ_schaefer500_retests.mat'\n",
    "    retest_subj_dir = master_dir + f'/{task.upper()}/subjvec_retest_DZ_schaefer500_retests.mat'\n",
    "    f = h5py.File(test_subj_dir, 'r')\n",
    "    for k, v in f.items():\n",
    "        test_subj_vec[k] = np.array(v)\n",
    "    f = h5py.File(retest_subj_dir, 'r')\n",
    "    for k, v in f.items():\n",
    "        retest_subj_vec[k] = np.array(v)\n",
    "    test_subj_ids[task] = test_subj_vec['subj_vec'].astype(int)\n",
    "    retest_subj_ids[task] = retest_subj_vec['subj_vec'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_twin1, retest_twin1 = {}, {}\n",
    "for task in tasks:\n",
    "    test_twin1[task]= set(test_subj_ids[task][0])\n",
    "    retest_twin1[task] = set(retest_subj_ids[task][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common twins over all tasks, test/retest: 58\n"
     ]
    }
   ],
   "source": [
    "# How many twin pairs are common between tasks?\n",
    "common_twins = set.intersection(test_twin1['rest'], test_twin1['emotion'], test_twin1['gambling'],test_twin1['language'], test_twin1['motor'],test_twin1['relational'], test_twin1['social'],test_twin1['wm'],\n",
    "                                retest_twin1['rest'], retest_twin1['emotion'], retest_twin1['gambling'],retest_twin1['language'], retest_twin1['motor'],retest_twin1['relational'], retest_twin1['social'],retest_twin1['wm'])\n",
    "num_common_twins = len(common_twins)\n",
    "print(f'Number of common twins over all tasks, test/retest: {num_common_twins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of each task only for the common twins\n",
    "test_twin_ind, retest_twin_ind = {}, {}\n",
    "for task in tasks:\n",
    "    test_twin_ind[task]= [2*i for i, val in enumerate(test_twin1[task]) if val in common_twins]\n",
    "    test_twin_ind[task] = test_twin_ind[task] + [x+1 for x in test_twin_ind[task]]\n",
    "    test_twin_ind[task].sort()\n",
    "    retest_twin_ind[task]= [2*i for i, val in enumerate(retest_twin1[task]) if val in common_twins] \n",
    "    retest_twin_ind[task] = retest_twin_ind[task] + [x+1 for x in retest_twin_ind[task]]\n",
    "    retest_twin_ind[task].sort()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get only the common FCs of twins\n",
    "common_FCs = get_data(test_twin_ind, retest_twin_ind, 100, twin='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.tile(np.repeat(np.arange(0,common_FCs['rest'].shape[0]/4),2),2)\n",
    "labels = labels.astype(int)\n",
    "train_idx = np.arange(0,common_FCs['rest'].shape[0],2)\n",
    "train_idx = train_idx.astype(int)\n",
    "test_idx = np.arange(1, int(common_FCs['rest'].shape[0]), 2)\n",
    "test_idx = test_idx.astype(int)\n",
    "train_labels = labels[train_idx]\n",
    "test_labels = labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFCs = common_FCs['rest'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100 region parcellation...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-ea4a126a40d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Using {parc} region parcellation...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcommon_FCs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_twin_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretest_twin_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_FCs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'rest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emotion'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gambling'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'language'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'motor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relational'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'social'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Analyzing {task}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracies = {}\n",
    "lengths = {100:6441, 200:22791, 300:49141, 400:85491, 500:131841}\n",
    "for parc in np.arange(100,400,100):\n",
    "    print(f'Using {parc} region parcellation...')\n",
    "    common_FCs = get_data(test_twin_ind, retest_twin_ind, parc, twin='D')\n",
    "    print(common_FCs[rest].shape)\n",
    "    for task in ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']:\n",
    "        print(f'Analyzing {task}...')\n",
    "        task_FCs = common_FCs[task]\n",
    "        # Do optional transformations\n",
    "        for ref in ['Raw FC']: #, 'pca', 'euclid', 'harmonic']:\n",
    "            # Start with a fresh batch of FCs\n",
    "            FC = np.zeros((task_FCs.shape[0], parc+14, parc+14))\n",
    "            for idx, utri in enumerate(task_FCs):\n",
    "                FC[idx] = utri2mat(utri)\n",
    "            # Do optional transformations\n",
    "            if ref != 'Raw FC' and ref != 'pca':\n",
    "                FC = tangential(FC, ref)\n",
    "            elif ref == 'pca':\n",
    "                print('Reconstructing with PCA')\n",
    "                FC = pca_recon(FC, 0.5)\n",
    "            else:\n",
    "                pass\n",
    "            # Convert back into flattened utriu vectors\n",
    "            vec_FCs = np.zeros(\n",
    "                (nFCs, lengths[parc]), dtype=np.float32)\n",
    "            for idx, mat in enumerate(FC):\n",
    "                vec_FCs[idx] = mat[np.triu_indices(mat.shape[0], k=1)]\n",
    "            # Split into train and test sets\n",
    "            train_FCs = vec_FCs[train_idx]\n",
    "            test_FCs = vec_FCs[test_idx]\n",
    "            # KNN Classifier\n",
    "            neigh = KNeighborsClassifier(n_neighbors=1, metric='correlation')\n",
    "            neigh.fit(train_FCs, train_labels)\n",
    "            predicted = neigh.predict(test_FCs)\n",
    "            acc = accuracy_score(test_labels, predicted)\n",
    "            print(f'{acc:.5f} accuracy')\n",
    "            accuracies[f\"{parc}:{task}\"] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "a_file = open(f\"../results/DZ_twin_parcellations.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in accuracies.items():\n",
    "    writer.writerow([key, value])\n",
    "    \n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
