{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import scipy\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "from scipy.spatial import distance\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utri2mat(utri):\n",
    "    n = int(-1 + np.sqrt(1 + 8 * len(utri))) // 2\n",
    "    iu1 = np.triu_indices(n+1,1)\n",
    "    ret = np.empty((n+1, n+1))\n",
    "    ret[iu1] = utri\n",
    "    ret.T[iu1] = utri\n",
    "    np.fill_diagonal(ret, 1)\n",
    "    return ret\n",
    "\n",
    "def get_data(test_idx, retest_idx, parc, twin='DZ'):\n",
    "    '''\n",
    "    Navigates through file tree and extracts FCs with optional reconstruction\n",
    "    '''\n",
    "    # Yeo ordering\n",
    "    master_dir = '../data/twins'\n",
    "    tasks = ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']\n",
    "    FC, test, retest = {}, {}, {}\n",
    "    for task in tasks:\n",
    "        temp_parc = {}\n",
    "        task_dir = master_dir + f'/{task.upper()}/origmat_{twin}_schaefer{parc}_tests.mat'\n",
    "        f = h5py.File(task_dir, 'r')\n",
    "        for k, v in f.items():\n",
    "            temp_parc[k] = np.array(v)\n",
    "        test[task] = temp_parc['orig_mat'][test_idx[task]]\n",
    "        temp_parc = {}\n",
    "        task_dir = master_dir + f'/{task.upper()}/origmat_{twin}_schaefer{parc}_retests.mat'\n",
    "        f = h5py.File(task_dir, 'r')\n",
    "        for k, v in f.items():\n",
    "            temp_parc[k] = np.array(v)\n",
    "        retest[task] = temp_parc['orig_mat'][retest_idx[task]]\n",
    "        FC[task] = np.concatenate((test[task], retest[task])) \n",
    "        FC[task] = test[task]\n",
    "    return FC\n",
    "\n",
    "\n",
    "def q1invm(q1, eig_thresh=0):\n",
    "    U, S, V = scipy.linalg.svd(q1)\n",
    "    s = np.diag(S)\n",
    "    s[s < eig_thresh] = eig_thresh\n",
    "    S = np.diag(s ** (-1 / 2))\n",
    "    Q1_inv_sqrt = U * S * np.transpose(V)\n",
    "    Q1_inv_sqrt = (Q1_inv_sqrt + np.transpose(Q1_inv_sqrt)) / 2\n",
    "    return Q1_inv_sqrt\n",
    "\n",
    "\n",
    "def qlog(q):\n",
    "    U, S, V = scipy.linalg.svd(q)\n",
    "    s = np.diag(S)\n",
    "    S = np.diag(np.log(s))\n",
    "    Q = U * S * np.transpose(V)\n",
    "    return Q\n",
    "\n",
    "\n",
    "def tangential(all_FC, ref):\n",
    "    # Regularization for riemann\n",
    "    if ref in ['riemann', 'kullback_sym', 'logeuclid']: \n",
    "        print(\"Adding regularization!\")\n",
    "        eye_mat = np.eye(all_FC.shape[1])\n",
    "        scaling_mat = np.repeat(eye_mat[None, ...], all_FC.shape[0], axis=0)\n",
    "        all_FC += scaling_mat\n",
    "    u, s, vh = np.linalg.svd(all_FC[0], full_matrices=True)\n",
    "    Cg = mean_covariance(all_FC, metric=ref)\n",
    "    Q1_inv_sqrt = q1invm(Cg)\n",
    "    Q = Q1_inv_sqrt @ all_FC @ Q1_inv_sqrt\n",
    "    tangent_FC = np.array([qlog(a) for a in Q])\n",
    "    return tangent_FC\n",
    "\n",
    "\n",
    "def pca_recon(FC, pctComp=None):\n",
    "    '''\n",
    "    Reconstructs FC based on number of principle components\n",
    "    '''\n",
    "    if pctComp is None:\n",
    "        return FC\n",
    "    nRegions = FC.shape[1]\n",
    "    FC = np.reshape(FC, (FC.shape[0], -1))\n",
    "    nComp = int(FC.shape[0] * pctComp)\n",
    "    mu = np.mean(FC, axis=0)\n",
    "    pca_rest = sklearn.decomposition.PCA()\n",
    "    pca_rest.fit(FC)\n",
    "    SCORES = pca_rest.transform(FC)[:, :nComp]\n",
    "    COEFFS = pca_rest.components_[:nComp, :]\n",
    "    FC_recon = np.dot(SCORES, COEFFS)\n",
    "    del SCORES, COEFFS\n",
    "    FC_recon += mu\n",
    "    FC_recon = np.reshape(FC_recon, (FC.shape[0], nRegions, nRegions))\n",
    "    return FC_recon\n",
    "\n",
    "def utri2mat(utri):\n",
    "    n = int(-1 + np.sqrt(1 + 8 * len(utri))) // 2\n",
    "    iu1 = np.tril_indices(n+1,-1)\n",
    "    ret = np.empty((n+1, n+1))\n",
    "    ret[iu1] = utri\n",
    "    ret.T[iu1] = utri\n",
    "    np.fill_diagonal(ret, 1)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twin Subject ID Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twin = \"MZ\"\n",
    "tasks = ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']\n",
    "test_subj_ids, retest_subj_ids = {}, {}\n",
    "for task in tasks:\n",
    "    test_subj_vec, retest_subj_vec = {}, {}\n",
    "    master_dir = '../data/twins'\n",
    "    test_subj_dir = master_dir + f'/{task.upper()}/subjvec_test_{twin}_schaefer300_retests.mat'\n",
    "    retest_subj_dir = master_dir + f'/{task.upper()}/subjvec_retest_{twin}_schaefer300_retests.mat'\n",
    "    f = h5py.File(test_subj_dir, 'r')\n",
    "    for k, v in f.items():\n",
    "        test_subj_vec[k] = np.array(v)\n",
    "    f = h5py.File(retest_subj_dir, 'r')\n",
    "    for k, v in f.items():\n",
    "        retest_subj_vec[k] = np.array(v)\n",
    "    test_subj_ids[task] = test_subj_vec['subj_vec'].astype(int)\n",
    "    retest_subj_ids[task] = retest_subj_vec['subj_vec'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_twin1, retest_twin1 = {}, {}\n",
    "for task in tasks:\n",
    "    test_twin1[task]= set(test_subj_ids[task][0])\n",
    "    retest_twin1[task] = set(retest_subj_ids[task][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common twins over all tasks, test/retest: 106\n"
     ]
    }
   ],
   "source": [
    "# How many twin pairs are common between tasks?\n",
    "common_twins = set.intersection(test_twin1['rest'], test_twin1['emotion'], test_twin1['gambling'],test_twin1['language'], test_twin1['motor'],test_twin1['relational'], test_twin1['social'],test_twin1['wm'],\n",
    "                                retest_twin1['rest'], retest_twin1['emotion'], retest_twin1['gambling'],retest_twin1['language'], retest_twin1['motor'],retest_twin1['relational'], retest_twin1['social'],retest_twin1['wm'])\n",
    "num_common_twins = len(common_twins)\n",
    "print(f'Number of common twins over all tasks, test/retest: {num_common_twins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of each task only for the common twins\n",
    "test_twin_ind, retest_twin_ind = {}, {}\n",
    "for task in tasks:\n",
    "    test_twin_ind[task]= [2*i for i, val in enumerate(test_twin1[task]) if val in common_twins]\n",
    "    test_twin_ind[task] = test_twin_ind[task] + [x+1 for x in test_twin_ind[task]]\n",
    "    test_twin_ind[task].sort()\n",
    "    retest_twin_ind[task]= [2*i for i, val in enumerate(retest_twin1[task]) if val in common_twins] \n",
    "    retest_twin_ind[task] = retest_twin_ind[task] + [x+1 for x in retest_twin_ind[task]]\n",
    "    retest_twin_ind[task].sort()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_FCs = get_data(test_twin_ind, retest_twin_ind, 100, twin=twin)\n",
    "labels = np.repeat(np.arange(0,common_FCs['rest'].shape[0]/2),2)\n",
    "labels = labels.astype(int)\n",
    "train_idx = np.arange(0,common_FCs['rest'].shape[0],2)\n",
    "train_idx = train_idx.astype(int)\n",
    "test_idx = np.arange(1, int(common_FCs['rest'].shape[0]), 2)\n",
    "test_idx = test_idx.astype(int)\n",
    "train_labels = labels[train_idx]\n",
    "test_labels = labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nFCs = common_FCs['rest'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Approach - Manual Tangent Space, all Parcellations, all Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing rest...\n",
      "Accuracies: 0.32075 and 0.36792\n",
      "Analyzing emotion...\n",
      "Accuracies: 0.09434 and 0.09434\n",
      "Analyzing gambling...\n",
      "Accuracies: 0.15094 and 0.18868\n",
      "Analyzing language...\n",
      "Accuracies: 0.29245 and 0.33019\n",
      "Analyzing motor...\n",
      "Accuracies: 0.07547 and 0.09434\n",
      "Analyzing relational...\n",
      "Accuracies: 0.08491 and 0.07547\n",
      "Analyzing social...\n",
      "Accuracies: 0.20755 and 0.14151\n",
      "Analyzing wm...\n",
      "Accuracies: 0.25472 and 0.21698\n",
      "Analyzing rest...\n",
      "Accuracies: 0.40566 and 0.39623\n",
      "Analyzing emotion...\n",
      "Accuracies: 0.10377 and 0.09434\n",
      "Analyzing gambling...\n",
      "Accuracies: 0.17925 and 0.25472\n",
      "Analyzing language...\n",
      "Accuracies: 0.39623 and 0.41509\n",
      "Analyzing motor...\n",
      "Accuracies: 0.10377 and 0.16038\n",
      "Analyzing relational...\n",
      "Accuracies: 0.15094 and 0.14151\n",
      "Analyzing social...\n",
      "Accuracies: 0.31132 and 0.19811\n",
      "Analyzing wm...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e25deb03c237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mneigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'correlation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_FCs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_FCs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0macc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_FCs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m                 **kwds))\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_method\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ball_tree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kd_tree'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0;32m-> 1592\u001b[0;31m                                      n_jobs=n_jobs, **kwds)\n\u001b[0m\u001b[1;32m   1593\u001b[0m         if ((X is Y or Y is None)\n\u001b[1;32m   1594\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetric_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2775\u001b[0m             XA, XB, typ, kwargs = _validate_cdist_input(XA, XB, mA, mB, n,\n\u001b[0;32m-> 2776\u001b[0;31m                                                         metric_name, **kwargs)\n\u001b[0m\u001b[1;32m   2777\u001b[0m             \u001b[0;31m# get cdist wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m             cdist_fn = getattr(_distance_wrap,\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_validate_cdist_input\u001b[0;34m(XA, XB, mA, mB, n, metric_name, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# validate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mXA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mXB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# validate kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36m_convert_to_type\u001b[0;34m(X, out_type)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_to_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fmri/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36mascontiguousarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracies = {}\n",
    "lengths = {100:6441, 200:22791, 300:49141, 400:85491, 500:131841}\n",
    "for parc in np.arange(100, 500, 100):\n",
    "    common_FCs = get_data(test_twin_ind, retest_twin_ind, parc, twin=twin)\n",
    "    for task in ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']:\n",
    "        print(f'Analyzing {task}...')\n",
    "        task_FCs = common_FCs[task]\n",
    "        # Do optional transformations\n",
    "        for ref in ['original']:\n",
    "            # Start with a fresh batch of FCs\n",
    "            FC = np.zeros((task_FCs.shape[0], parc+14, parc+14))\n",
    "            for idx, utri in enumerate(task_FCs):\n",
    "                FC[idx] = utri2mat(utri)\n",
    "            # Do optional transformations\n",
    "            if ref != 'original' and ref != 'pca':\n",
    "                FC = tangential(FC, ref)\n",
    "                with open(f'../data/twins/{twin}_{parc}_{ref}', 'wb') as f:\n",
    "                    pickle.dump(FC, f)\n",
    "                print(\"saved common FCs\")\n",
    "            elif ref == 'pca':\n",
    "                print('Reconstructing with PCA')\n",
    "                FC = pca_recon(FC, 0.3)\n",
    "            else:\n",
    "                pass\n",
    "            # Convert back into flattened utriu vectors\n",
    "            vec_FCs = np.zeros(\n",
    "                (nFCs, lengths[parc]), dtype=np.float32)\n",
    "            for idx, mat in enumerate(FC):\n",
    "                vec_FCs[idx] = mat[np.triu_indices(mat.shape[0], k=1)]\n",
    "            # Split into train and test sets\n",
    "            train_FCs = vec_FCs[train_idx]\n",
    "            test_FCs = vec_FCs[test_idx]\n",
    "            # KNN Classifier\n",
    "            neigh = KNeighborsClassifier(n_neighbors=1, metric='correlation')\n",
    "            neigh.fit(train_FCs, train_labels)\n",
    "            predicted = neigh.predict(test_FCs)\n",
    "            acc1 = accuracy_score(test_labels, predicted)\n",
    "            neigh.fit(test_FCs, test_labels)\n",
    "            predicted = neigh.predict(train_FCs)\n",
    "            acc2 = accuracy_score(test_labels, predicted)\n",
    "            print(f'Accuracies: {acc1:.5f} and {acc2:.5f}')\n",
    "            accuracies[f\"{parc}:{task}:{ref}\"] = (acc1 + acc2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save common tangent projected FCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing rest...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing emotion...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing gambling...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing language...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing motor...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing relational...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing social...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing wm...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing rest...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing emotion...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing gambling...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing language...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing motor...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing relational...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing social...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing wm...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing rest...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing emotion...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing gambling...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing language...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing motor...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing relational...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing social...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing wm...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing rest...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing emotion...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing gambling...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing language...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing motor...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing relational...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing social...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n",
      "Analyzing wm...\n",
      "saved euclid\n",
      "saved harmonic\n",
      "Adding regularization!\n",
      "saved kullback_sym\n",
      "Adding regularization!\n",
      "saved logeuclid\n",
      "Adding regularization!\n",
      "saved riemann\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracies = {}\n",
    "lengths = {100:6441, 200:22791, 300:49141, 400:85491, 500:131841}\n",
    "for parc in np.arange(100, 500, 100):\n",
    "    common_FCs = get_data(test_twin_ind, retest_twin_ind, parc, twin=twin)\n",
    "    for task in ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']:\n",
    "        print(f'Analyzing {task}...')\n",
    "        task_FCs = common_FCs[task]\n",
    "        # Do optional transformations\n",
    "        for ref in ['euclid', 'harmonic', 'kullback_sym', 'logeuclid', 'riemann']:\n",
    "            # Start with a fresh batch of FCs\n",
    "            FC = np.zeros((task_FCs.shape[0], parc+14, parc+14))\n",
    "            for idx, utri in enumerate(task_FCs):\n",
    "                FC[idx] = utri2mat(utri)\n",
    "            # Do optional transformations\n",
    "            FC = tangential(FC, ref)\n",
    "            with open(f'../data/tangent_fcs/twins/common/{task}/{twin}_{parc}_{ref}.pickle', 'wb') as f:\n",
    "                pickle.dump(FC, f)\n",
    "            print(f\"saved {ref}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100:rest:original': 0.3443396226415094,\n",
       " '100:emotion:original': 0.09433962264150944,\n",
       " '100:gambling:original': 0.169811320754717,\n",
       " '100:language:original': 0.3113207547169811,\n",
       " '100:motor:original': 0.0849056603773585,\n",
       " '100:relational:original': 0.08018867924528301,\n",
       " '100:social:original': 0.17452830188679247,\n",
       " '100:wm:original': 0.2358490566037736,\n",
       " '200:rest:original': 0.4009433962264151,\n",
       " '200:emotion:original': 0.09905660377358491,\n",
       " '200:gambling:original': 0.2169811320754717,\n",
       " '200:language:original': 0.4056603773584906,\n",
       " '200:motor:original': 0.1320754716981132,\n",
       " '200:relational:original': 0.14622641509433962,\n",
       " '200:social:original': 0.25471698113207547,\n",
       " '200:wm:original': 0.3160377358490566,\n",
       " '300:rest:original': 0.4339622641509434,\n",
       " '300:emotion:original': 0.10377358490566038,\n",
       " '300:gambling:original': 0.25943396226415094,\n",
       " '300:language:original': 0.42924528301886794,\n",
       " '300:motor:original': 0.12735849056603774,\n",
       " '300:relational:original': 0.17452830188679247,\n",
       " '300:social:original': 0.3018867924528302,\n",
       " '300:wm:original': 0.33018867924528306,\n",
       " '400:rest:original': 0.4481132075471698,\n",
       " '400:emotion:original': 0.12264150943396226,\n",
       " '400:gambling:original': 0.24056603773584906,\n",
       " '400:language:original': 0.4811320754716981,\n",
       " '400:motor:original': 0.12735849056603774,\n",
       " '400:relational:original': 0.1792452830188679,\n",
       " '400:social:original': 0.32075471698113206,\n",
       " '400:wm:original': 0.34905660377358494}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "a_file = open(f\"../results/twins/{twin}_twin_parcellations_new.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in accuracies.items():\n",
    "    writer.writerow([key, value])\n",
    "    \n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tangent Space FCs - load premade tangent FCs, all parcellations, all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100 region parcellation...\n",
      "Analyzing rest...\n",
      "0.05660 accuracy\n",
      "Analyzing emotion...\n",
      "0.01887 accuracy\n",
      "Analyzing gambling...\n",
      "0.03774 accuracy\n",
      "Analyzing language...\n",
      "0.04717 accuracy\n",
      "Analyzing motor...\n",
      "0.03774 accuracy\n",
      "Analyzing relational...\n",
      "0.00000 accuracy\n",
      "Analyzing social...\n",
      "0.02830 accuracy\n",
      "Analyzing wm...\n",
      "0.04717 accuracy\n",
      "Using 200 region parcellation...\n",
      "Analyzing rest...\n",
      "0.01887 accuracy\n",
      "Analyzing emotion...\n",
      "0.00943 accuracy\n",
      "Analyzing gambling...\n",
      "0.01887 accuracy\n",
      "Analyzing language...\n",
      "0.00943 accuracy\n",
      "Analyzing motor...\n",
      "0.00943 accuracy\n",
      "Analyzing relational...\n",
      "0.02830 accuracy\n",
      "Analyzing social...\n",
      "0.06604 accuracy\n",
      "Analyzing wm...\n",
      "0.01887 accuracy\n",
      "Using 300 region parcellation...\n",
      "Analyzing rest...\n",
      "0.00943 accuracy\n",
      "Analyzing emotion...\n",
      "0.00943 accuracy\n",
      "Analyzing gambling...\n",
      "0.00943 accuracy\n",
      "Analyzing language...\n",
      "0.04717 accuracy\n",
      "Analyzing motor...\n",
      "0.04717 accuracy\n",
      "Analyzing relational...\n",
      "0.00943 accuracy\n",
      "Analyzing social...\n",
      "0.00943 accuracy\n",
      "Analyzing wm...\n",
      "0.02830 accuracy\n",
      "Using 400 region parcellation...\n",
      "Analyzing rest...\n",
      "0.04717 accuracy\n",
      "Analyzing emotion...\n",
      "0.02830 accuracy\n",
      "Analyzing gambling...\n",
      "0.00000 accuracy\n",
      "Analyzing language...\n",
      "0.03774 accuracy\n",
      "Analyzing motor...\n",
      "0.02830 accuracy\n",
      "Analyzing relational...\n",
      "0.02830 accuracy\n",
      "Analyzing social...\n",
      "0.01887 accuracy\n",
      "Analyzing wm...\n",
      "0.02830 accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracies = {}\n",
    "twin = 'MZ'\n",
    "lengths = {100:6441, 200:22791, 300:49141, 400:85491, 500:131841}\n",
    "for parc in np.arange(100,500,100):\n",
    "    print(f'Using {parc} region parcellation...')\n",
    "    for task in ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']:\n",
    "        print(f'Analyzing {task}...')\n",
    "        for ref in ['logeuclid']:\n",
    "            # Start with a fresh batch of FCs\n",
    "            with open(f'../data/tangent_fcs/twins/common/{task}/{twin}_{parc}_{ref}.pickle', 'rb') as f:\n",
    "                FC = pickle.load(f)\n",
    "            # Convert back into flattened utriu vectors\n",
    "            vec_FCs = np.zeros((nFCs, lengths[parc]), dtype=np.float32)\n",
    "            for idx, mat in enumerate(FC):\n",
    "                vec_FCs[idx] = mat[np.triu_indices(mat.shape[0], k=1)]\n",
    "            # Split into train and test sets\n",
    "            train_FCs = vec_FCs[train_idx]\n",
    "            test_FCs = vec_FCs[test_idx]\n",
    "            # KNN Classifier\n",
    "            neigh = KNeighborsClassifier(n_neighbors=1, metric='correlation')\n",
    "            neigh.fit(train_FCs, train_labels)\n",
    "            predicted = neigh.predict(test_FCs)\n",
    "            acc = accuracy_score(test_labels, predicted)\n",
    "            print(f'{acc:.5f} accuracy')\n",
    "            accuracies[f\"{parc}:{task}:{ref}\"] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "a_file = open(f\"../results/twins/{twin}_twin_logeuclid.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in accuracies.items():\n",
    "    writer.writerow([key, value])\n",
    "    \n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
