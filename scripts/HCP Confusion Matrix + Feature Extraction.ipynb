{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=5)\n",
    "        self.conv2_bn = nn.BatchNorm2d(12)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(18252, 128)\n",
    "        self.fc2 = nn.Linear(128, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (3, 3)))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_bn(self.conv2(x)), (3, 3)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "def get_data(method):\n",
    "    '''\n",
    "    Navigates through file tree and extracts FCs with optional reconstruction\n",
    "    '''\n",
    "    fname = '../data/100_unrelated.csv'\n",
    "    yeo = True\n",
    "    if yeo:\n",
    "        yeo_order = list(sio.loadmat(\"../data/yeo_RS7_N374.mat\",\n",
    "                                     squeeze_me=True,\n",
    "                                     struct_as_record=False)['yeoOrder'] - 1)\n",
    "    subjectids = np.loadtxt(fname, dtype=np.int)\n",
    "    nSubj = len(subjectids)\n",
    "    tasks = ['rfMRI_REST1_LR', 'rfMRI_REST1_RL', 'rfMRI_REST2_LR',\n",
    "             'rfMRI_REST2_RL', 'tfMRI_EMOTION_LR', 'tfMRI_EMOTION_RL',\n",
    "             'tfMRI_GAMBLING_LR', 'tfMRI_GAMBLING_RL', 'tfMRI_LANGUAGE_LR',\n",
    "             'tfMRI_LANGUAGE_RL', 'tfMRI_MOTOR_LR', 'tfMRI_MOTOR_RL',\n",
    "             'tfMRI_RELATIONAL_LR', 'tfMRI_RELATIONAL_RL', 'tfMRI_SOCIAL_LR',\n",
    "             'tfMRI_SOCIAL_RL', 'tfMRI_WM_LR', 'tfMRI_WM_RL']\n",
    "\n",
    "    M = {}\n",
    "    for task in tasks:\n",
    "        masterFC_dir = '../data/results_SIFT2'\n",
    "        restingstatename = 'fMRI/' + task + '/FC/FC_glasser_subc_GS_bp_z.mat'\n",
    "        task_matrices = []\n",
    "        for subject in subjectids:\n",
    "            filename = masterFC_dir + '/' + \\\n",
    "                str(subject) + '/' + restingstatename\n",
    "            mat = sio.loadmat(filename, squeeze_me=True,\n",
    "                              struct_as_record=False)\n",
    "            A_orig = mat['FC']\n",
    "            if yeo:\n",
    "                A_orig = A_orig[np.ix_(yeo_order, yeo_order)]\n",
    "            np.fill_diagonal(A_orig, 1)\n",
    "            task_matrices.append(A_orig)\n",
    "        M[task] = np.array(task_matrices)\n",
    "    test = np.concatenate((M['rfMRI_REST1_LR'], M['tfMRI_EMOTION_LR'],\n",
    "                           M['tfMRI_GAMBLING_LR'], M['tfMRI_LANGUAGE_LR'],\n",
    "                           M['tfMRI_MOTOR_LR'], M['tfMRI_RELATIONAL_LR'],\n",
    "                           M['tfMRI_SOCIAL_LR'], M['tfMRI_WM_LR']))\n",
    "    retest = np.concatenate((M['rfMRI_REST1_RL'], M['tfMRI_EMOTION_RL'],\n",
    "                             M['tfMRI_GAMBLING_RL'], M['tfMRI_LANGUAGE_RL'],\n",
    "                             M['tfMRI_MOTOR_RL'], M['tfMRI_RELATIONAL_RL'],\n",
    "                             M['tfMRI_SOCIAL_RL'], M['tfMRI_WM_RL']))\n",
    "    del M\n",
    "    all_FC = np.float32(np.concatenate((test, retest)))\n",
    "    del test, retest\n",
    "    tangent_FC = tangential(all_FC, method)\n",
    "    return tangent_FC, nSubj\n",
    "\n",
    "\n",
    "def q1invm(q1, eig_thresh=0):\n",
    "    U, S, V = scipy.linalg.svd(q1)\n",
    "    s = np.diag(S)\n",
    "    s[s < eig_thresh] = eig_thresh\n",
    "    S = np.diag(s ** (-1 / 2))\n",
    "    Q1_inv_sqrt = U * S * np.transpose(V)\n",
    "    Q1_inv_sqrt = (Q1_inv_sqrt + np.transpose(Q1_inv_sqrt)) / 2\n",
    "    return Q1_inv_sqrt\n",
    "\n",
    "\n",
    "def qlog(q):\n",
    "    U, S, V = scipy.linalg.svd(q)\n",
    "    s = np.diag(S)\n",
    "    S = np.diag(np.log(s))\n",
    "    Q = U * S * np.transpose(V)\n",
    "    return Q\n",
    "\n",
    "\n",
    "def tangential(all_FC, method):\n",
    "    Cg = mean_covariance(all_FC, metric=method)\n",
    "    Q1_inv_sqrt = q1invm(Cg)\n",
    "    Q = Q1_inv_sqrt @ all_FC @ Q1_inv_sqrt\n",
    "    tangent_FC = np.array([qlog(a) for a in Q])\n",
    "    return tangent_FC\n",
    "\n",
    "\n",
    "def prepare_data(all_FC, nSubj):\n",
    "    '''\n",
    "    Prepares labels and train, val and test data from raw data\n",
    "    '''\n",
    "    # Create labels corresponding to all_FC\n",
    "    labels = torch.tensor(\n",
    "        np.tile(np.repeat(np.arange(0, 8), nSubj), 2), dtype=torch.long)\n",
    "    # Randomly shuffled indices for test FCs\n",
    "    indices = np.random.permutation(all_FC.shape[0])\n",
    "    # Take subsets of data for training, validation, test\n",
    "    train_val_idx = indices[:int(0.8 * all_FC.shape[0])]\n",
    "    # Val, train, test indices\n",
    "    val_idx = train_val_idx[int(0.8 * train_val_idx.shape[0]):]\n",
    "    train_idx = train_val_idx[:int(0.8 * train_val_idx.shape[0])]\n",
    "    test_idx = indices[int(0.8 * all_FC.shape[0]):]\n",
    "\n",
    "    train_mean = np.mean(all_FC[train_idx])\n",
    "    train_std = np.std(all_FC[train_idx])\n",
    "    train_data = torch.FloatTensor(\n",
    "        (all_FC[train_idx] - train_mean) / train_std)\n",
    "    val_data = torch.FloatTensor((all_FC[val_idx] - train_mean) / train_std)\n",
    "    test_data = torch.FloatTensor((all_FC[test_idx] - train_mean) / train_std)\n",
    "\n",
    "    train_data = train_data.view(\n",
    "        train_data.shape[0], -1, train_data.shape[1], train_data.shape[2])\n",
    "    val_data = val_data.view(\n",
    "        val_data.shape[0], -1, val_data.shape[1], val_data.shape[2])\n",
    "    test_data = test_data.view(\n",
    "        test_data.shape[0], -1, test_data.shape[1], test_data.shape[2])\n",
    "\n",
    "    train_dataset = data.TensorDataset(\n",
    "        train_data, labels[train_idx])  # create your datset\n",
    "    val_dataset = data.TensorDataset(\n",
    "        val_data, labels[val_idx])  # create your datset\n",
    "    test_dataset = data.TensorDataset(\n",
    "        test_data, labels[test_idx])  # create your datset\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset, batch_size=80)  # create your dataloader\n",
    "    val_loader = data.DataLoader(\n",
    "        val_dataset, batch_size=80)  # create your dataloader\n",
    "    test_loader = data.DataLoader(\n",
    "        test_dataset, batch_size=80)  # create your dataloader\n",
    "\n",
    "    return train_loader, val_loader, test_loader, labels[test_idx]\n",
    "\n",
    "\n",
    "def build_model(lr):\n",
    "    '''\n",
    "    Given layer sizes and learning rate, builds model.\n",
    "    Can change NN architecture here directly in nn.Sequential\n",
    "    '''\n",
    "    model = Net()\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "    history = {}\n",
    "    history['loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['acc'] = []\n",
    "    history['val_acc'] = []\n",
    "    return model, loss_fn, opt, history\n",
    "\n",
    "\n",
    "def train_model(model, opt, loss_fn, train_loader, val_loader,\n",
    "                max_epochs, n_epochs_stop, history):\n",
    "    '''\n",
    "    Trains model with specified parameters and returns trained model\n",
    "    '''\n",
    "    early_stop = False\n",
    "    min_val_loss = np.Inf\n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        num_train_correct = 0\n",
    "        num_train_examples = 0\n",
    "        for local_batch, local_labels in train_loader:\n",
    "            # Transfer to GPU\n",
    "            if use_cuda:\n",
    "                local_batch, local_labels = local_batch.to(\n",
    "                    device), local_labels.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output = model(local_batch)\n",
    "            loss = loss_fn(output, local_labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss += loss.data.item() * local_batch.size(0)\n",
    "            num_train_correct += (torch.max(output, 1)\n",
    "                                  [1] == local_labels).sum().item()\n",
    "            num_train_examples += local_batch.shape[0]\n",
    "\n",
    "            train_acc = num_train_correct / num_train_examples\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        num_val_correct = 0\n",
    "        num_val_examples = 0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for local_batch, local_labels in val_loader:\n",
    "                # Transfer to GPU\n",
    "                if use_cuda:\n",
    "                    local_batch, local_labels = local_batch.to(\n",
    "                        device), local_labels.to(device)\n",
    "                output = model(local_batch)\n",
    "                loss = loss_fn(output, local_labels)\n",
    "\n",
    "                val_loss += loss.data.item() * local_batch.size(0)\n",
    "                num_val_correct += (torch.max(output, 1)\n",
    "                                    [1] == local_labels).sum().item()\n",
    "                num_val_examples += local_batch.shape[0]\n",
    "\n",
    "            val_acc = num_val_correct / num_val_examples\n",
    "            val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "                epochs_no_improve = 0\n",
    "                min_val_loss = val_loss\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "        # Check early stopping condition\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                early_stop = print('Early stopping!')\n",
    "                break\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} / {max_epochs},'\n",
    "                  f'train loss: {train_loss: 5.4f},'\n",
    "                  f'train acc: {train_acc: 5.3f}, val loss: {val_loss: 5.3f},'\n",
    "                  f'val acc: {val_acc: 5.3f}')\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        if early_stop:\n",
    "            print(\"Stopped\")\n",
    "            break\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    '''\n",
    "    After trained model is returned, this function tests the accuracy\n",
    "    on novel data. Returns test accuracy of a model.\n",
    "    '''\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_examples = 0\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in test_loader:\n",
    "            # Transfer to GPU\n",
    "            if use_cuda:\n",
    "                local_batch, local_labels = local_batch.to(\n",
    "                    device), local_labels.to(device)\n",
    "            output = model(local_batch)\n",
    "            \n",
    "            num_correct += (torch.max(output, 1)\n",
    "                            [1] == local_labels).sum().item()\n",
    "            num_examples += local_batch.shape[0]\n",
    "\n",
    "        test_acc = num_correct / num_examples\n",
    "    return test_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter reference matrix for regularization: euclid\n",
      "GPU detected. Will use GPU for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\anaconda3\\envs\\env_full\\lib\\site-packages\\ipykernel_launcher.py:92: RuntimeWarning: divide by zero encountered in power\n",
      "C:\\Users\\Michael\\anaconda3\\envs\\env_full\\lib\\site-packages\\ipykernel_launcher.py:101: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# GPU is available? If so, we use it.\n",
    "reference_mats = ['riemann', 'logeuclid', 'euclid', 'identity',\n",
    "                  'logdet', 'wasserstein', 'ale', 'harmonic',\n",
    "                  'kullback_sym']\n",
    "method = None\n",
    "while method not in reference_mats:\n",
    "    method = input(\"Enter reference matrix for regularization: \")\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    print(\"GPU detected. Will use GPU for training!\")\n",
    "    torch.backends.cudnn\n",
    "    benchmark = True\n",
    "else:\n",
    "    print(\"No GPU detected. Will use CPU for training.\")\n",
    "tangent_FC, nSubj = get_data(method)\n",
    "replicates = np.arange(1, 2)\n",
    "all_acc, all_loss = {}, {}\n",
    "# Get data from file tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, validation, and test data for NN\n",
    "train_loader, val_loader, test_loader, test_labels = prepare_data(\n",
    "    tangent_FC, nSubj)\n",
    "# Max epochs of training, early stopping threshold, learning rate\n",
    "max_epochs, n_epochs_stop, lr = 200, 5, 0.001\n",
    "# Build model accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training...\n",
      "Epoch 1 / 200,train loss:  0.0214,train acc:  0.303, val loss:  1.936,val acc:  0.344\n",
      "Epoch 11 / 200,train loss:  0.0056,train acc:  0.779, val loss:  0.914,val acc:  0.619\n",
      "Epoch 21 / 200,train loss:  0.0027,train acc:  0.865, val loss:  0.710,val acc:  0.725\n",
      "Epoch 31 / 200,train loss:  0.0016,train acc:  0.918, val loss:  0.606,val acc:  0.783\n",
      "Epoch 41 / 200,train loss:  0.0011,train acc:  0.951, val loss:  0.541,val acc:  0.799\n",
      "Epoch 51 / 200,train loss:  0.0007,train acc:  0.962, val loss:  0.494,val acc:  0.840\n",
      "Epoch 61 / 200,train loss:  0.0006,train acc:  0.974, val loss:  0.459,val acc:  0.852\n",
      "Epoch 71 / 200,train loss:  0.0004,train acc:  0.981, val loss:  0.431,val acc:  0.861\n",
      "Epoch 81 / 200,train loss:  0.0003,train acc:  0.986, val loss:  0.408,val acc:  0.865\n",
      "Epoch 91 / 200,train loss:  0.0003,train acc:  0.990, val loss:  0.390,val acc:  0.869\n",
      "Epoch 101 / 200,train loss:  0.0002,train acc:  0.992, val loss:  0.374,val acc:  0.869\n",
      "Epoch 111 / 200,train loss:  0.0002,train acc:  0.992, val loss:  0.361,val acc:  0.873\n",
      "Epoch 121 / 200,train loss:  0.0002,train acc:  0.997, val loss:  0.350,val acc:  0.869\n",
      "Epoch 131 / 200,train loss:  0.0001,train acc:  0.998, val loss:  0.341,val acc:  0.877\n",
      "Epoch 141 / 200,train loss:  0.0001,train acc:  0.998, val loss:  0.333,val acc:  0.885\n",
      "Epoch 151 / 200,train loss:  0.0001,train acc:  0.998, val loss:  0.326,val acc:  0.877\n",
      "Epoch 161 / 200,train loss:  0.0001,train acc:  0.998, val loss:  0.320,val acc:  0.881\n",
      "Epoch 171 / 200,train loss:  0.0001,train acc:  1.000, val loss:  0.315,val acc:  0.881\n",
      "Epoch 181 / 200,train loss:  0.0001,train acc:  1.000, val loss:  0.310,val acc:  0.869\n",
      "Epoch 191 / 200,train loss:  0.0001,train acc:  1.000, val loss:  0.306,val acc:  0.869\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for sequence element 0 in sequence argument at position #1 'tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7e02a184ec6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                              \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs_stop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                              history)\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Test accuracy of model is {accuracy}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f4d88e8642f9>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m    275\u001b[0m                     device), local_labels.to(device)\n\u001b[0;32m    276\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[0mall_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             num_correct += (torch.max(output, 1)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for sequence element 0 in sequence argument at position #1 'tensors'"
     ]
    }
   ],
   "source": [
    "\n",
    "model, loss_fn, opt, history = build_model(lr)\n",
    "print(f\"Now training...\")\n",
    "model, history = train_model(model, opt, loss_fn, train_loader,\n",
    "                             val_loader, max_epochs, n_epochs_stop,\n",
    "                             history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of model is 0.9111842105263158\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(model, test_loader)\n",
    "print(f'Test accuracy of model is {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack((test_labels, all_preds.argmax(dim=1)),dim=1)\n",
    "stacked.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
