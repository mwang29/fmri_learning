{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import scipy\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from pyriemann.utils.mean import mean_covariance\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "from scipy.spatial import distance\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utri2mat(utri):\n",
    "    n = int(-1 + np.sqrt(1 + 8 * len(utri))) // 2\n",
    "    iu1 = np.triu_indices(n+1,1)\n",
    "    ret = np.empty((n+1, n+1))\n",
    "    ret[iu1] = utri\n",
    "    ret.T[iu1] = utri\n",
    "    np.fill_diagonal(ret, 1)\n",
    "    return ret\n",
    "\n",
    "def get_data(parc, twin='DZ'):\n",
    "    '''\n",
    "    Navigates through file tree and extracts test/retest FCs \n",
    "    '''\n",
    "    master_dir = '../data/twins'\n",
    "    tasks = ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']\n",
    "    FC, test, retest = {}, {}, {}\n",
    "    for task in tasks:\n",
    "        task_dir = master_dir + f'/{task.upper()}/origmat_{twin}_schaefer{parc}_tests.mat'\n",
    "        f = h5py.File(task_dir, 'r')\n",
    "        for k, v in f.items():\n",
    "            test[task] = np.array(v)\n",
    "        task_dir = master_dir + f'/{task.upper()}/origmat_{twin}_schaefer{parc}_retests.mat'\n",
    "        f = h5py.File(task_dir, 'r')\n",
    "        for k, v in f.items():\n",
    "            retest[task] = np.array(v)\n",
    "        FC[task] = np.concatenate((test[task], retest[task])) \n",
    "    return FC\n",
    "\n",
    "def pca_recon(FC, pctComp=None):\n",
    "    '''\n",
    "    Reconstructs FC based on number of principle components\n",
    "    '''\n",
    "    if pctComp is None:\n",
    "        return FC\n",
    "    nRegions = FC.shape[1]\n",
    "    FC = np.reshape(FC, (FC.shape[0], -1))\n",
    "    nComp = int(FC.shape[0] * pctComp)\n",
    "    mu = np.mean(FC, axis=0)\n",
    "    pca_rest = sklearn.decomposition.PCA()\n",
    "    pca_rest.fit(FC)\n",
    "    SCORES = pca_rest.transform(FC)[:, :nComp]\n",
    "    COEFFS = pca_rest.components_[:nComp, :]\n",
    "    FC_recon = np.dot(SCORES, COEFFS)\n",
    "    del SCORES, COEFFS\n",
    "    FC_recon += mu\n",
    "    FC_recon = np.reshape(FC_recon, (FC.shape[0], nRegions, nRegions))\n",
    "    return FC_recon\n",
    "\n",
    "def utri2mat(utri):\n",
    "    n = int(-1 + np.sqrt(1 + 8 * len(utri))) // 2\n",
    "    iu1 = np.tril_indices(n+1,-1)\n",
    "    ret = np.empty((n+1, n+1))\n",
    "    ret[iu1] = utri\n",
    "    ret.T[iu1] = utri\n",
    "    np.fill_diagonal(ret, 1)\n",
    "    return ret\n",
    "\n",
    "def get_schaefer(parc, ref='original'):\n",
    "    if ref.lower() == 'original' or ref.lower() == 'geodesic':\n",
    "        with open(f'../data/schaefer/schaefer{parc}.pickle', 'rb') as f:\n",
    "            all_FC = pickle.load(f)\n",
    "    else:\n",
    "        with open(f'../data/tangent_fcs/schaefer/schaefer{parc}_{ref}.pickle', 'rb') as f:\n",
    "            all_FC = pickle.load(f)\n",
    "    nSubj = int(all_FC.shape[0]/16)\n",
    "    return all_FC, nSubj\n",
    "\n",
    "def get_task_fcs(parc, ref='original'):\n",
    "    '''\n",
    "    Outputs a dictionary of each group of task FCs\n",
    "    '''\n",
    "    FCs, nSubj = get_schaefer(parc, ref)\n",
    "    taskFCs = {}\n",
    "    n = 0\n",
    "    for task in ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']:\n",
    "        taskFCs[task] = np.concatenate((FCs[n:n+424], FCs[n+3392:n+3392+424]))\n",
    "        n += 424\n",
    "    return taskFCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate each task into separate subsets\n",
    "FCs, nSubj = get_schaefer(100)\n",
    "taskFCs = get_task_fcs(100)\n",
    "nFCs = taskFCs['rest'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label each subject\n",
    "labels = np.tile(np.arange(0, nSubj), 2)\n",
    "labels = labels.astype(int)\n",
    "train_idx = np.arange(0,taskFCs['rest'].shape[0]/2)\n",
    "train_idx = train_idx.astype(int)\n",
    "test_idx = np.arange(taskFCs['rest'].shape[0]/2, int(taskFCs['rest'].shape[0]))\n",
    "test_idx = test_idx.astype(int)\n",
    "train_labels = labels[train_idx]\n",
    "test_labels = labels[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracies = {}\n",
    "lengths = {100:6441, 200:22791, 300:49141, 400:85491, 500:131841}\n",
    "for parc in np.arange(100, 500, 100): \n",
    "    for ref in ['original', 'euclid', 'harmonic', 'kullback_sym', 'logeuclid', 'riemann']:\n",
    "        print(f'Analyzing {parc} with {ref}...')\n",
    "        FCs = get_task_fcs(parc, ref)\n",
    "        for task in ['rest', 'emotion', 'gambling', 'language', 'motor', 'relational', 'social', 'wm']:\n",
    "            task_FCs = FCs[task]\n",
    "            # Convert back into flattened utriu vectors\n",
    "            vec_FCs = np.zeros((nFCs, lengths[parc]), dtype=np.float32)\n",
    "            for idx, mat in enumerate(task_FCs):\n",
    "                vec_FCs[idx] = mat[np.triu_indices(mat.shape[0], k=1)]\n",
    "            # Split into train and test sets\n",
    "            train_FCs = vec_FCs[train_idx]\n",
    "            test_FCs = vec_FCs[test_idx]\n",
    "            # KNN Classifier\n",
    "            neigh = KNeighborsClassifier(n_neighbors=1, metric='correlation')\n",
    "            neigh.fit(train_FCs, train_labels)\n",
    "            predicted = neigh.predict(test_FCs)\n",
    "            acc1 = accuracy_score(test_labels, predicted)\n",
    "            neigh.fit(test_FCs, test_labels)\n",
    "            predicted = neigh.predict(train_FCs)\n",
    "            acc2 = accuracy_score(test_labels, predicted)\n",
    "            print(f'{task}: {acc1:.5f} and {acc2:.5f}')\n",
    "            accuracies[f\"{parc}:{task}:{ref}\"] = (acc1 + acc2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "a_file = open(f\"../results/subject/subject_identification.csv\", \"w\")\n",
    "\n",
    "writer = csv.writer(a_file)\n",
    "for key, value in accuracies.items():\n",
    "    writer.writerow([key, value])\n",
    "    \n",
    "a_file.close()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
